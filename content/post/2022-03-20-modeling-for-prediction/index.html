---
title: "Modeling for Prediction"
author: "R package build"
date: '2022-03-20'
slug: modeling-for-prediction
categories: []
tags: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="using-information-from-the-following-website" class="section level2">
<h2>Using Information from the following website</h2>
<pre class="r"><code>link &lt;- url(&quot;https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set&quot;)</code></pre>
</div>
<div id="github-link-to-dataset" class="section level2">
<h2>Github link to dataset</h2>
<pre class="r"><code>link2 &lt;- url(&quot;https://github.com/angelacalicchio/a01-real-estate/tree/main&quot;)</code></pre>
</div>
<div id="questions-and-answers" class="section level1">
<h1>Questions and Answers</h1>
<blockquote>
<p>What are the advantages and disadvantages of k-fold cross validation relative to Single Split Validation set approach and LOOCV?</p>
</blockquote>
<p>Single Split Validation Advantages<br />
+ Very easy and simple to understand<br />
+ Splits data in a way that is easy to implement</p>
<p>Single Split Validation Disadvantages<br />
+ Can create a large error rate depending on location of split<br />
+ Gives up valuable data, since not all the data will be used</p>
<p>LOOCV Advantages<br />
+ Great for small datasets<br />
+ Contains zero randomness(doesn’t generate random numbers)<br />
+ Diminishes bias, due to using the entire dataset(creates a more accurate average)</p>
<p>LOOCV Disadvantages<br />
+ Computationally expensive, bad for large datasets</p>
<blockquote>
<p>Discuss the Pros and Cons to Bootstrapping.</p>
</blockquote>
<p>Pros<br />
+ Good for situations in which the sample size is small and the sampling distribution cannot be assumed as normal<br />
+ Good when it is too difficult to work out the standard error of the estimates<br />
+ Generates samples <strong>with replacement</strong> which imitates parametric approaches in statistics</p>
<p>Cons<br />
+ Cannot be used if the dataset uses data that cannot be replaced, for example, if the data is a time series, it cannot be observed with replacement<br />
+ Each bootstrap sample has significant overlap with the original data, which will cause the bootstrap to seriously underestimate the true prediction error<br />
+ The problem of duplicates and overlaps can be fixed, but then bootstrapping is too complicated</p>
<div id="setup" class="section level2">
<h2>Setup</h2>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──</code></pre>
<pre><code>## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4
## ✓ tibble  3.1.6     ✓ dplyr   1.0.7
## ✓ tidyr   1.2.0     ✓ stringr 1.4.0
## ✓ readr   2.1.2     ✓ forcats 0.5.1</code></pre>
<pre><code>## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(boot)
library(readxl)
library(emmeans)</code></pre>
</div>
<div id="loading-the-data-from-excel" class="section level2">
<h2>Loading the Data from Excel</h2>
<pre class="r"><code>valuation &lt;- read_excel(&quot;~/Downloads/Real estate valuation data set.xlsx&quot;)
valuation &lt;- valuation[,-1]
View(valuation)
summary(valuation)</code></pre>
<pre><code>##  X1 transaction date  X2 house age    X3 distance to the nearest MRT station
##  Min.   :2013        Min.   : 0.000   Min.   :  23.38                       
##  1st Qu.:2013        1st Qu.: 9.025   1st Qu.: 289.32                       
##  Median :2013        Median :16.100   Median : 492.23                       
##  Mean   :2013        Mean   :17.713   Mean   :1083.89                       
##  3rd Qu.:2013        3rd Qu.:28.150   3rd Qu.:1454.28                       
##  Max.   :2014        Max.   :43.800   Max.   :6488.02                       
##  X4 number of convenience stores  X5 latitude     X6 longitude  
##  Min.   : 0.000                  Min.   :24.93   Min.   :121.5  
##  1st Qu.: 1.000                  1st Qu.:24.96   1st Qu.:121.5  
##  Median : 4.000                  Median :24.97   Median :121.5  
##  Mean   : 4.094                  Mean   :24.97   Mean   :121.5  
##  3rd Qu.: 6.000                  3rd Qu.:24.98   3rd Qu.:121.5  
##  Max.   :10.000                  Max.   :25.01   Max.   :121.6  
##  Y house price of unit area
##  Min.   :  7.60            
##  1st Qu.: 27.70            
##  Median : 38.45            
##  Mean   : 37.98            
##  3rd Qu.: 46.60            
##  Max.   :117.50</code></pre>
</div>
</div>
<div id="method-1-cross-validation" class="section level1">
<h1>Method 1: Cross Validation</h1>
<div id="set-seed" class="section level2">
<h2>Set Seed</h2>
<pre class="r"><code># To reproduce the sampling for train - test split
set.seed(1)
head(valuation)</code></pre>
<pre><code>## # A tibble: 6 × 7
##   `X1 transaction date` `X2 house age` `X3 distance to the ne…` `X4 number of …`
##                   &lt;dbl&gt;          &lt;dbl&gt;                    &lt;dbl&gt;            &lt;dbl&gt;
## 1                 2013.           32                       84.9               10
## 2                 2013.           19.5                    307.                 9
## 3                 2014.           13.3                    562.                 5
## 4                 2014.           13.3                    562.                 5
## 5                 2013.            5                      391.                 5
## 6                 2013.            7.1                   2175.                 3
## # … with 3 more variables: `X5 latitude` &lt;dbl&gt;, `X6 longitude` &lt;dbl&gt;,
## #   `Y house price of unit area` &lt;dbl&gt;</code></pre>
<pre class="r"><code>dim(valuation)</code></pre>
<pre><code>## [1] 414   7</code></pre>
<pre class="r"><code>## Create an index to randomly sample 50% records from Auto
train &lt;- sample(414, 7)</code></pre>
<div id="rename-the-columns-for-easier-integration" class="section level3">
<h3>Rename the Columns for Easier Integration</h3>
<pre class="r"><code>colnames(valuation)[colnames(valuation) == &quot;X1 transaction date&quot;] &lt;- &quot;x1&quot;
colnames(valuation)[colnames(valuation) == &quot;X2 house age&quot;] &lt;- &quot;x2&quot;
colnames(valuation)[colnames(valuation) == &quot;X3 distance to the nearest MRT station&quot;] &lt;- &quot;x3&quot;
colnames(valuation)[colnames(valuation) == &quot;X4 number of convenience stores&quot;] &lt;- &quot;x4&quot;
colnames(valuation)[colnames(valuation) == &quot;X5 latitude&quot;] &lt;- &quot;x5&quot;
colnames(valuation)[colnames(valuation) == &quot;X6 longitude&quot;] &lt;- &quot;x6&quot;
colnames(valuation)[colnames(valuation) == &quot;Y house price of unit area&quot;] &lt;- &quot;x7&quot;
# I had to change &quot;y&quot; to &quot;x7&quot;, otherwise it was perceiving it as a date</code></pre>
</div>
</div>
<div id="make-the-variables-in-auto-dataset-as-locally-accessible-objects" class="section level2">
<h2>Make the variables in Auto dataset as locally accessible objects</h2>
<pre class="r"><code>attach(valuation)
lm.fit &lt;- lm(x7~x3, data = valuation, subset = train)
lm.fit</code></pre>
<pre><code>## 
## Call:
## lm(formula = x7 ~ x3, data = valuation, subset = train)
## 
## Coefficients:
## (Intercept)           x3  
##    53.52692     -0.01043</code></pre>
</div>
<div id="find-the-mean" class="section level2">
<h2>Find the Mean</h2>
<pre class="r"><code>mean((x7 - predict(lm.fit, valuation))[-train]^2)</code></pre>
<pre><code>## [1] 134.1401</code></pre>
</div>
<div id="plot-it" class="section level2">
<h2>Plot It</h2>
<pre class="r"><code>plot(x7~x3)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="fit-a-quadratic-function" class="section level2">
<h2>Fit a quadratic function</h2>
<pre class="r"><code>lm.fit.poly &lt;- lm(x7~poly(x3,2), data = valuation, subset = train)
lm.fit.poly</code></pre>
<pre><code>## 
## Call:
## lm(formula = x7 ~ poly(x3, 2), data = valuation, subset = train)
## 
## Coefficients:
##  (Intercept)  poly(x3, 2)1  poly(x3, 2)2  
##        41.11       -208.00        183.29</code></pre>
<p><strong>As we increase the degree of the polynomial to 2, the error decreases</strong></p>
<pre class="r"><code>mean((x7 - predict(lm.fit.poly, valuation))[-train]^2)</code></pre>
<pre><code>## [1] 126.5326</code></pre>
<div id="trying-a-different-polynomial" class="section level3">
<h3>Trying a different polynomial</h3>
<pre class="r"><code>lm.fit.poly.3 &lt;- lm(x7~poly(x3,3), data = valuation, subset = train)
lm.fit.poly.3</code></pre>
<pre><code>## 
## Call:
## lm(formula = x7 ~ poly(x3, 3), data = valuation, subset = train)
## 
## Coefficients:
##  (Intercept)  poly(x3, 3)1  poly(x3, 3)2  poly(x3, 3)3  
##        35.55       -465.67       -205.12       -380.45</code></pre>
<pre class="r"><code>lm.fit.poly.4 &lt;- lm(x7~poly(x3,4), data = valuation, subset = train)
lm.fit.poly.4</code></pre>
<pre><code>## 
## Call:
## lm(formula = x7 ~ poly(x3, 4), data = valuation, subset = train)
## 
## Coefficients:
##  (Intercept)  poly(x3, 4)1  poly(x3, 4)2  poly(x3, 4)3  poly(x3, 4)4  
##        -78.8       -9740.2      -13837.6      -12719.2       -3803.4</code></pre>
<p>In this instance it seems that the best polynomial for this specific quadratic equation would be 2, since the values given by 3 and 4 are not great results for the data we are trying to collect</p>
</div>
</div>
<div id="set-a-new-seed-and-compare" class="section level2">
<h2>Set a New Seed and Compare</h2>
<pre class="r"><code>set.seed(10)
train &lt;- sample(414, 7)
lm.fit &lt;- lm(x7~x3, data = valuation, subset = train)
lm.fit</code></pre>
<pre><code>## 
## Call:
## lm(formula = x7 ~ x3, data = valuation, subset = train)
## 
## Coefficients:
## (Intercept)           x3  
##    65.12443     -0.01354</code></pre>
<pre class="r"><code>lm.fit.poly &lt;- lm(x7~poly(x3,2), data = valuation, subset = train)
mean((x7 - predict(lm.fit, valuation))[-train]^2)</code></pre>
<pre><code>## [1] 314.1497</code></pre>
<pre class="r"><code>mean((x7 - predict(lm.fit.poly, valuation))[-train]^2)</code></pre>
<pre><code>## [1] 696.112</code></pre>
</div>
</div>
<div id="method-2-loo-cv-leave-one-out-cross-validation" class="section level1">
<h1>Method 2: LOO CV: Leave One Out Cross Validation</h1>
<pre class="r"><code>## GLM defaults to lm when passed without any arguments
glm.fit &lt;- glm(x7~x3, data = valuation)
coef(glm.fit)</code></pre>
<pre><code>##  (Intercept)           x3 
## 45.851427058 -0.007262052</code></pre>
<pre class="r"><code>lm.fit &lt;- lm(x7~x3, data = valuation)
coef(lm.fit)</code></pre>
<pre><code>##  (Intercept)           x3 
## 45.851427058 -0.007262052</code></pre>
<pre class="r"><code>cv.err &lt;- cv.glm(valuation, glm.fit)
cv.err$delta</code></pre>
<pre><code>## [1] 101.8171 101.8160</code></pre>
<pre class="r"><code>cv.error &lt;- rep(0, 5)
degree &lt;- 1:5
for (d in degree){
  glm.fit &lt;- glm(x7~poly(x3,d), data = valuation)
  cv.error[d] &lt;- cv.glm(valuation, glm.fit)$delta[1]
}
cv.error</code></pre>
<pre><code>## [1] 101.81709  88.70438  83.09502  81.12731  81.60174</code></pre>
<pre class="r"><code>plot(degree, cv.error, type = &quot;b&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
<div id="k-fold-cross-validation" class="section level1">
<h1>K Fold Cross Validation</h1>
<pre class="r"><code>K = 10
cv.error.10 &lt;- rep(0, 5)
degree &lt;- 1:5
for (d in degree){
  glm.fit &lt;- glm(x7~poly(x3,d), data = valuation)
  cv.error.10[d] &lt;- cv.glm(valuation, glm.fit, K = K)$delta[1]
}
cv.error.10</code></pre>
<pre><code>## [1] 102.09638  88.45879  82.88173  80.95370  81.40123</code></pre>
<pre class="r"><code>plot(degree, cv.error, type = &quot;b&quot;)
lines(degree, cv.error.10, type = &quot;b&quot;, col = &quot;red&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<div id="lets-try-a-different-k-values" class="section level3">
<h3>Let’s try a different k values</h3>
<pre class="r"><code>K = 5
cv.error.5 &lt;- rep(0, 5)
degree &lt;- 1:5
for (d in degree){
  glm.fit &lt;- glm(x7~poly(x3,d), data = valuation)
  cv.error.5[d] &lt;- cv.glm(valuation, glm.fit, K = K)$delta[1]
}
cv.error.5</code></pre>
<pre><code>## [1] 102.16544  87.80528  83.10011  80.61283  81.12042</code></pre>
<pre class="r"><code>plot(degree, cv.error, type = &quot;b&quot;)
lines(degree, cv.error.5, type = &quot;b&quot;, col = &quot;blue&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code>K = 2
cv.error.2 &lt;- rep(0, 5)
degree &lt;- 1:5
for (d in degree){
  glm.fit &lt;- glm(x7~poly(x3,d), data = valuation)
  cv.error.2[d] &lt;- cv.glm(valuation, glm.fit, K = K)$delta[1]
}
cv.error.2</code></pre>
<pre><code>## [1] 102.87599  89.98011  82.73082  84.22101  82.00967</code></pre>
<pre class="r"><code>plot(degree, cv.error, type = &quot;b&quot;)
lines(degree, cv.error.2, type = &quot;b&quot;, col = &quot;green&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<blockquote>
<p>What was learned?</p>
</blockquote>
<ul>
<li>The best fit k value in this instance is 10</li>
</ul>
</div>
<div id="trying-different-degrees" class="section level3">
<h3>Trying different degrees</h3>
<pre class="r"><code>cv.error.degree.change &lt;- rep(0, 10)
degree &lt;- 1:10
for (d in degree){
  glm.fit &lt;- glm(x7~poly(x3,d), data = valuation)
  cv.error.degree.change[d] &lt;- cv.glm(valuation, glm.fit)$delta[1]
}
cv.error.degree.change</code></pre>
<pre><code>##  [1] 101.81709  88.70438  83.09502  81.12731  81.60174  80.10651  80.44117
##  [8]  80.72772  80.69415  81.15789</code></pre>
<pre class="r"><code>K = 10
cv.error.degree.10 &lt;- rep(0, 10)
degree &lt;- 1:10
for (d in degree){
  glm.fit &lt;- glm(x7~poly(x3,d), data = valuation)
  cv.error.10[d] &lt;- cv.glm(valuation, glm.fit, K = K)$delta[1]
}
cv.error.degree.10</code></pre>
<pre><code>##  [1] 0 0 0 0 0 0 0 0 0 0</code></pre>
<div id="lets-try-a-lower-degree" class="section level4">
<h4>Let’s try a lower degree</h4>
<pre class="r"><code>K = 10
cv.error.degree.2 &lt;- rep(0, 2)
degree &lt;- 1:2
for (d in degree){
  glm.fit &lt;- glm(x7~poly(x3,d), data = valuation)
  cv.error.2[d] &lt;- cv.glm(valuation, glm.fit, K = K)$delta[1]
}
cv.error.degree.2</code></pre>
<pre><code>## [1] 0 0</code></pre>
<blockquote>
<p>What could this mean?</p>
</blockquote>
<ul>
<li>Perhaps the best k value is 10 and the best degree is 5, for this situation</li>
</ul>
</div>
</div>
</div>
<div id="bootstrap-validation" class="section level1">
<h1>Bootstrap Validation</h1>
<pre class="r"><code>## Estimation of Accuracy of a Linear Model
boot.fn &lt;- function(data, index){
  return(coef(lm(x7~x3, data = data, subset = index)))
}</code></pre>
<pre class="r"><code>boot.fn(valuation, 1:414)</code></pre>
<pre><code>##  (Intercept)           x3 
## 45.851427058 -0.007262052</code></pre>
<pre class="r"><code>set.seed(1)
boot.fn(valuation, sample(414, 414, replace = T))</code></pre>
<pre><code>## (Intercept)          x3 
## 47.53654707 -0.00769632</code></pre>
<pre class="r"><code>boot.out &lt;- boot(valuation, boot.fn, 1000)</code></pre>
<pre class="r"><code>plot(boot.out)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<div id="lets-see-what-happens-when-adding-a-larger-sample-size" class="section level3">
<h3>Let’s see what happens when adding a larger sample size</h3>
<pre class="r"><code>boot.out &lt;- boot(valuation, boot.fn, 10000)</code></pre>
<pre class="r"><code>plot(boot.out)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<pre class="r"><code>boot.out &lt;- boot(valuation, boot.fn, 100000)</code></pre>
<pre class="r"><code>plot(boot.out)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p>The larger the sample size, the more the data resembles a normal distribution. However, in order to generate this large of a sample set, the program needed much more time.</p>
</div>
</div>
